# -*- coding: utf-8 -*-
"""Finals season 3.1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tYvcgbKEW03bwL2zu4CwUIjt2c3O-Kg_
"""

!pip install pymssql

import pandas as pd
import pymssql
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Server: 45.117.83.230 - Port : 1433
# Account : Student_DA_Q1 - Table: [dbo].[CarPrice_Assignment]
# Password: @MindXDream2023
host = "45.117.83.230"
port = 1433
user = "Student_DA_Q1"
password = "@MindXDream2023"
connection = pymssql.connect(host = host, port =port , user = user, password = password)
query = "SELECT * FROM [dbo].[CarPrice_Assignment]"
df_carprice = pd.read_sql(query, connection)
df_carprice.head()

from google.colab import drive
drive.mount('/content/drive')

df_carprice.to_csv("/content/drive/My Drive/carprice1.csv", index = False)

df = pd.read_csv("/content/drive/My Drive/carprice1.csv")

# Separate CarName to brand and model
df[['brand', 'model']] = df['CarName'].str.split(' ', n= 1, expand=True)
df.drop(["CarName"], axis=1, inplace=True)
df.head(5)

unique_counts=df.nunique()
print("Number of unique data points in each column:")
print(unique_counts)

#df.head(5)
#df.tail(5)
df.info()
#df.describe() #chỉ xem biến có số
#df.shape #a bit too fat, but we can remove some features
#df.isnull().sum() # check null
#df.duplicated().sum() #check duplicate rowa

# use boxplot for price analysis of brands
plt.figure(figsize=(12,10), dpi=100)
sns.boxplot(x=df["brand"], y=df["price"])
plt.xticks(rotation=45)
# -> outlier is screwing the data --> need to remove the outliers
# --> we can also see that there are some mispelling in brand name --> replace values

#define function to remove outlier using IQR
def remove_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.1)
    Q3 = data[column].quantile(0.9)
    IQR = Q3 - Q1
    lower_bound = Q1 - 2.4 * IQR
    upper_bound = Q3 - (-2.4 * IQR)
    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]
df = remove_outliers_iqr(df, 'price')

# fixing mispelled brand names
#maxda - mazda, Nissan - nissan, toyota - toyouta, vokswagen - volkswagen - vw, porcshce - porsche
df = df.replace({'brand': {'vokswagen': 'volkswagen', 'vw': 'volkswagen','maxda': 'mazda','toyouta': 'toyota', 'Nissan': 'nissan', 'porcshce':'porsche'}})

unique_counts=df.nunique()
print("Number of unique data points in each column:")
print(unique_counts)

df.info()

# use boxplot for price analysis of brands
plt.figure(figsize=(12,10), dpi=100)
sns.boxplot(x=df["brand"], y=df["price"])
plt.xticks(rotation=45)
# we can see that there is definately some correlation between brands and price, there maybe segment of high/mid/lower price range brands

#Quick barplot for categorical data, showing how many data points belong to each category (or how diverse is our data)
cols1 = df.iloc[:,[2,3,4,5,6,7,13,14,16,25]]
for i in cols1.columns:
    plt.figure(figsize=(3,4),dpi=100)
    cols1[i].value_counts().plot.bar()

#Quick boxplot for numerical data
cols2 = df.iloc[:, [1,8,9,10,11,12,15,17,18,19,20,21,22,23]]
for i in cols2.columns:
  plt.figure(figsize=(6,6), dpi=100)
  sns.boxplot(data=df[i])

# use boxplot for price analysis of categorical data
cols3 = df.iloc[:,[2,3,4,5,6,7,13,14,16,25]]
for i in cols3:
  plt.figure(figsize=(6,6), dpi=100)
  sns.boxplot(x=df[i], y=df["price"])

#correlation_matrix for numerical data
cols4 = df.iloc[:, [1,8,9,10,11,12,15,17,18,19,20,21,22,23,24]]
correlation_matrix = cols4.corr()
carprice_corr = correlation_matrix['price']
plt.figure(figsize=(20, 6))
sns.barplot(x=carprice_corr.index, y=carprice_corr.values, palette='viridis')

plt.figure(figsize=(10,10))
sns.heatmap(cols4.corr(), annot=True, cmap='coolwarm')

# Selecting features to train
df_train = df.iloc[:,[1,8,9,10,11,12,15,17,18,19,20,21,22,23,24]]
df_train.head()

#from sklearn.preprocessing import StandardScaler

#df_train = StandardScaler().fit_transform(df_1)
#df_train = pd.DataFrame(df_train, columns=df_1.columns)
#df_train.head()

#Chọn dữ liệu X và y
X=df_train.iloc[:,0:14]
y=df_train.price
#Chia dữ liệu thành training set và test set
from sklearn.model_selection import train_test_split # Thư viện hỗ trợ chia data thành train và test set
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=5)

from sklearn.linear_model import LinearRegression # Hồi quy tuyến tính
from sklearn.tree import DecisionTreeRegressor  #Cây hồi quy
from sklearn.ensemble import RandomForestRegressor #RF hồi quy
from sklearn.svm import SVR # SVR
from sklearn.neighbors import KNeighborsRegressor #KNN
from sklearn.metrics import mean_squared_error, mean_absolute_error #Tính toán độ chính xác
from sklearn.metrics import r2_score
import math
import datetime #Tính thời gian
#Tạo danh sách chứa tên mô hình: CHÚ Ý CÁC HYPERPARAMETERS CHƯA ĐƯỢC CÀI ĐẶT TỐI ƯU, ĐANG ĐỂ MẶC ĐỊNH (Chúng ta có thể tìm hyperparameter riêng ở ngoải rồi cài đặt vào từng mô hình)
models = [LinearRegression(),
    DecisionTreeRegressor(max_depth=4,random_state=1),
    RandomForestRegressor(max_depth=8, max_features='sqrt', n_estimators=9,random_state=1),
    SVR(C= 100, epsilon=100, kernel= 'rbf'),
    KNeighborsRegressor(n_neighbors=4)]

#Tạo list rỗng chứa kết quả của mô hình
mae_ = []
rmse_ = []
time =[]
name=[]
r2_=[]
for model in models:
  t1 = datetime.datetime.now()
  model_name = model.__class__.__name__
  model.fit(X_train, y_train)
  t2= datetime.datetime.now()
  d = round((t2-t1).microseconds/1000,1) #Tính thời gian
  y_pred = model.predict(X_test)
  mae = round(mean_absolute_error(y_test, y_pred),2)
  rmse = round(math.sqrt(mean_squared_error(y_test, y_pred)),2)
  r2=round(r2_score(y_test, y_pred),2)
  mae_.append(mae)
  rmse_.append(rmse)
  r2_.append(r2)
  time.append(d)
  name.append(model_name)

#Đưa kết quả thành một bảng tổng hợp.
Results = pd.DataFrame(np.column_stack([name, mae_, rmse_,r2_,time]),columns=['Name', 'MAE','RMSE','R-square','Time (s)'])
Results

from sklearn import tree
# DecisionTreeRegressor k-tuning (maxdepth)
score_totalT = pd.DataFrame(columns=['max_depth','mae','rmse','r2'], index=[0])
# Sử dụng vòng lặp for để tìm max_depth
for i in range(1,20):
  model_tree = tree.DecisionTreeRegressor(max_depth=i,random_state=1)
  model_tree.fit(X_train, y_train)
  y_pred=model_tree.predict(X_test)
  mae1 = round(mean_absolute_error(y_test, y_pred),2)
  rmse1 = round(math.sqrt(mean_squared_error(y_test, y_pred)),2)
  r21=round(r2_score(y_test, y_pred),2)
  score_totalT.loc[-1] = [i, mae1, rmse1, r21]
  score_totalT.index = score_totalT.index + 1
  score_totalT = score_totalT.sort_index()
score_totalT.head(20)

# Chúng ta có thể vẽ lại cây quyết định như sau
from sklearn import tree
model = tree.DecisionTreeRegressor(max_depth=4,random_state=1)
model = model.fit(X_train, y_train)#Tính toán/fit model
import graphviz #Thư viện để hiển thị hình ảnh
dot_data = tree.export_graphviz(model, out_file=None,feature_names=['symboling', 'wheelbase','carlength','carwidth','carheight','curbweight', 'enginesize', 'boreratio','stroke','compressionratio','horsepower','peakrpm', 'citympg', 'highwaympg'],class_names=['price'],filled=True, rounded=True, special_characters=True)
graph = graphviz.Source(dot_data)
graph

# RandomForestRegressor k-tuning (n_estimators,max_depth,max_features)
from sklearn.model_selection import GridSearchCV
grid_space = {
              'n_estimators':[1,5,9,13,17,21],
              'max_depth': [1,2,4,5,7,8,10],
              'max_features': ['auto', 'sqrt', 'log2']
              }
forest_reg = RandomForestRegressor(random_state=1)
rf_grid = GridSearchCV(forest_reg, param_grid=grid_space, scoring='neg_mean_squared_log_error')

rf_grid.fit(X_train, y_train)
rf_grid.best_params_

#Random forest
model_RF = RandomForestRegressor(max_depth = 13, max_features = 'sqrt', n_estimators = 21, random_state=1)
model_RF.fit(X_train, y_train)
y_pred = model_RF.predict(X_test)
mae3 = round(mean_absolute_error(y_test, y_pred),2)
rmse3 = round(math.sqrt(mean_squared_error(y_test, y_pred)),2)
r23=round(r2_score(y_test, y_pred),2)
print (mae3,rmse3,r23)

#Linear Regression
model_LR = LinearRegression()
model_LR.fit(X_train, y_train)
y_pred = model_LR.predict(X_test)
mae3 = round(mean_absolute_error(y_test, y_pred),2)
rmse3 = round(math.sqrt(mean_squared_error(y_test, y_pred)),2)
r23=round(r2_score(y_test, y_pred),2)
print('W=',model_LR.coef_)
print('w0=',model_LR.intercept_)
print (mae3,rmse3,r23)

#Linear Regression feature selection
no_features = len(X_train.iloc[:1,:].values.flatten().tolist())
for i in range(1,no_features):
  model_LR = LinearRegression()
  model_LR.fit(X_train.iloc[:,:i], y_train)
  y_pred = model_LR.predict(X_test.iloc[:,:i])
  mae3 = round(mean_absolute_error(y_test, y_pred),2)
  rmse3 = round(math.sqrt(mean_squared_error(y_test, y_pred)),2)
  r23=round(r2_score(y_test, y_pred),2)
  print (mae3,rmse3,r23)

# SVR
modelSVR = SVR(C= 100, epsilon=100, kernel= 'rbf')
modelSVR.fit(X_train, y_train)
y_pred = modelSVR.predict(X_test)
mae = round(mean_absolute_error(y_test, y_pred),2)
rmse = round(math.sqrt(mean_squared_error(y_test, y_pred)),2)
r2=round(r2_score(y_test, y_pred),2)
print(mae, rmse, r2)

# SVR k-tuning (C,gamma)
from sklearn.model_selection import GridSearchCV
grid_space = {
    'C': [0.001, 0.01, 0.1, 1,10, 100],
    'epsilon' : [0.001, 0.01,0.1,1,10,100],
    'kernel': ['rbf']
}
SVR_reg = SVR()
SVR_grid = GridSearchCV(SVR_reg, param_grid=grid_space, scoring='neg_mean_squared_error')

SVR_grid.fit(X_train, y_train)
SVR_grid.best_params_

# KNN regressor k-tuning (n_neighbors)
mae_KNNR = []
rmse_KNNR = []
n_neighbors=[]
r2_KNNR=[]
for i in range(1,20):
  model_KNNR = KNeighborsRegressor(n_neighbors=i)
  model_KNNR.fit(X_train, y_train)
  y_pred = model_KNNR.predict(X_test)
  mae = round(mean_absolute_error(y_test, y_pred),2)
  rmse = round(math.sqrt(mean_squared_error(y_test, y_pred)),2)
  r2=round(r2_score(y_test, y_pred),2)
  mae_KNNR.append(mae)
  rmse_KNNR.append(rmse)
  r2_KNNR.append(r2)
  n_neighbors.append(i)
#Đưa kết quả thành một bảng tổng hợp.
ResultsKNNR = pd.DataFrame(np.column_stack([n_neighbors, mae_KNNR, rmse_KNNR,r2_KNNR]),columns=['n_neighbors', 'MAE','RMSE','R-square'])
ResultsKNNR